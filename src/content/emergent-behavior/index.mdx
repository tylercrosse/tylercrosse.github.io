---
title: 'Language Models'
description: 'From n-grams to ChatGPT, how language models work and how they can be used to solve real-world problems.'
date: '2023-05-21'
tags: ['python', 'short', 'nlp', 'data-science', 'seedling-ðŸŒ±']
path: '/ideas/language-models-emergent'
draft: true
audience: 'All'
---

## Predictions & Emergent behavior

Fundamentally language models are just a bunch of numbers. When a large language model creates a prediction it is still doing it one word at a time in the same fashion as the n-gram model. Yet the predictions of the big models are uncanny. Spooky enough to elicit suggestions that the model is somehow sentient. How is this possible?

There is a rift in the 

For one the length of the text that larger models use for predictions has gotten much larger. Often called the context window, has grown from a few words in n-gram models to hundreds. This larger context window allows for better recognition of patterns in the text. For example, if you're reading a book and the author mentions that it's raining outside you can predict that the characters will need an umbrella if they go outside.

![Grommit laying train tracks](./grommit-train-tracks.jpeg)
